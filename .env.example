# --- EDGAR Project Environment Variables ---
# This is an example file. For the application to work, you must:
# 1. Copy this file to a new file named ".env" in the same directory.
# 2. Customize the paths below for your system if desired.

# --- Core Paths (Required) ---
# Directory where all downloaded files (zips, json) will be stored.
# Use absolute paths to avoid ambiguity. The script will create this directory.
# Example for Windows: DOWNLOAD_DIR=C:\data\edgar_project\downloads
# Example for Linux/macOS: DOWNLOAD_DIR=/data/edgar_project/downloads
DOWNLOAD_DIR=/path/to/your/edgar_data/downloads

# Path to the DuckDB database file. The file will be created if it doesn't exist.
# The directory containing the file must be writable.
# Example for Windows: DB_FILE=C:\data\edgar_project\db\edgar_analytics.duckdb
# Example for Linux/macOS: DB_FILE=/data/edgar_project/db/edgar_analytics.duckdb
DB_FILE=/path/to/your/edgar_data/edgar_analytics.duckdb

# --- SEC EDGAR API Settings (Required) ---
# The SEC requires a custom User-Agent for all API requests.
# Format: "YourCompanyName YourAppName YourContactEmail@example.com"
SEC_USER_AGENT="PersonalResearchProject your.email@example.com"


# --- Optional Script-Specific Settings ---
# You can uncomment and change these to control the behavior of specific scripts.

# === parse_to_parquet.py ===
# Limit the number of CIKs to process. Useful for testing. Comment out to process all.
# PROCESS_LIMIT=50
# Process only one specific CIK. Overrides PROCESS_LIMIT. Comment out for normal runs.
# PROCESS_SPECIFIC_CIK=0000320193
# Set the number of CIKs to parse before loading a batch to the database (default: 100).
# CIK_BATCH_SIZE=100

# --- General Parallel Processing Settings ---
# Number of concurrent workers for various CPU/IO-bound tasks (e.g., JSON extraction, JSON parsing).
# Default: 8. Set to 1 for single-threaded processing.
# MAX_CPU_IO_WORKERS=8
 
# === YFinance Settings (stock_data_gatherer.py & stock_info_gatherer.py) ===
# CRITICAL: YFinance gathering is DISABLED by default due to EXTREMELY severe rate limits.
# Rate limits can trigger after just a few requests and persist for hours.
# Only enable for very small, prioritized batches with long delays.
YFINANCE_DISABLED=1

# WARNING: Rate limits reset unpredictably (1-24 hours). Test before gathering.
# To test: Wait 6+ hours, then run: python tests/test_yfinance_simple.py

# Enable minimal connectivity test when YFINANCE_DISABLED=1 (default: 0).
# YFINANCE_MINIMAL=0

# Maximum number of retries for failed YFinance requests (default: 5).
# Note: Retries won't help with rate limits - you must wait them out.
# YFINANCE_MAX_RETRIES=3

# Base delay in seconds for exponential backoff when rate limit is hit (default: 15.0).
# For actual gathering, use 60s+ to avoid triggering limits.
# YFINANCE_BASE_DELAY=60.0


# --- API Keys (Required for certain gatherers) ---
# === macro_data_gatherer.py ===
# API Key for Federal Reserve Economic Data (FRED).
# Get a free key from: https://fred.stlouisfed.org/docs/api/api_key.html
# Required for gather-macro command.
# FRED_API_KEY=your_fred_api_key_here

# Optional: Delay between FRED API calls in seconds (default: 0.5)
# FRED_API_DELAY=0.5

# === Polygon/Massive.com Settings (stock_data_gatherer_polygon.py & ticker_info_gatherer_polygon.py) ===
# API Key for Massive.com (formerly Polygon.io) stock market data.
# Get a free key from: https://massive.com/ (polygon.io keys still work)
# Free tier: 5 API calls/minute, unlimited daily calls, 1-day delayed data
# POLYGON_API_KEY=your_polygon_api_key_here

# Optional: Maximum runtime for overnight Polygon stock gatherer runs in hours (default: 15)
# Set based on your needs - rate limiting means ~3-5 calls/min = ~180-300 jobs/hour
# POLYGON_MAX_RUNTIME_HOURS=15

# Optional: Clamp backlog intervals to a recent window (in years, default: 5)
# This limits historical range per job to reduce API strain (e.g., 1 = last ~365 days)
# POLYGON_CLAMP_LOOKBACK_YEARS=5

# Optional: Maximum number of workers for Polygon gathering (default: 1, recommended for free tier)
# POLYGON_MAX_WORKERS=1

# Optional: Max runtime hours for ticker info pipeline (default: 4)
# POLYGON_INFO_MAX_RUNTIME_HOURS=4

# Optional: Re-fetch ticker info if older than this many days (default: 30)
# POLYGON_INFO_REFRESH_DAYS=30

# Optional: Rate limit in calls per minute (default: 3, max: 5 for free tier)
# Lower = more conservative, less likely to hit 429s. Higher = faster but riskier.
# POLYGON_CALLS_PER_MINUTE=3

# Optional: Unified batch size for Polygon gatherers (default: 100)
# Controls how many rows are buffered before writing a parquet batch.
# Larger values reduce small-file churn; smaller values surface data sooner.
# POLYGON_BATCH_SIZE=100


# --- Performance & Tuning ---
# DuckDB memory limit for heavy operations like loading and indexing (default: 8GB).
# DUCKDB_MEMORY_LIMIT=8GB

# Optional: A directory for DuckDB to spill to disk if the memory limit is exceeded.
# Only needed for very large datasets or memory-constrained systems.
# DUCKDB_TEMP_DIR=/path/to/your/duckdb_temp
