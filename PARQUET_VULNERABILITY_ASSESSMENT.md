# Parse-to-Parquet Vulnerability Assessment and Mitigations

## Date: November 19, 2025

## Executive Summary

This document outlines vulnerabilities identified in the parse-to-parquet data pipeline and the implemented safeguards to address them.

---

## Identified Vulnerabilities

### 1. **Partial Batch Write Failures** (HIGH RISK)

**Description**: If the parquet writer process crashes mid-batch, some files may be written while others fail, leaving the parquet directory in an inconsistent state.

**Impact**:
- Incomplete data for a batch of CIKs
- Potential duplicate data if the process is restarted
- Database may contain partial information for some companies

**Example Scenario**:
```
Batch processing 100 CIKs:
- Companies parquet: ✓ Written (50 records)
- Tickers parquet: ✓ Written (150 records)  
- Filings parquet: ✗ CRASH
- XBRL data: ✗ Not written

Result: 50 companies with tickers but no filings or XBRL data
```

**Mitigation**: 
- Implemented `ParquetBatchManager` with atomic batch operations
- All writes for a batch are tracked and can be rolled back on failure
- Each batch gets a unique ID to identify related files
- Validation after each write to catch corruption early

### 2. **Failed Cleanup Operations** (MEDIUM RISK)

**Description**: The `clean_obsolete_parquet_files()` function assumes specific file naming patterns that don't match the actual batch file naming scheme.

**Current Code Issue**:
```python
# In parquet_converter_cleanup.py - expects files like "CIK0000320193.parquet"
for file_path in companies_dir.glob("*.parquet"):
    cik = file_path.stem.replace("CIK", "")
    if cik not in valid_ciks:
        os.remove(file_path)

# Actual files: "batch_20251119_143052_123456.parquet"
# Result: Cleanup doesn't work, obsolete files accumulate
```

**Impact**:
- Obsolete parquet files accumulate over time
- Wasted disk space
- Potential confusion when debugging data issues
- May load stale data into database

**Mitigation**:
- Created `cleanup_partial_batches()` function that understands batch naming
- Identifies incomplete batches by timestamp and file count
- Only deletes files older than configurable threshold (default 24 hours)
- Logs all cleanup operations for audit trail

### 3. **No Post-Write Validation** (MEDIUM RISK)

**Description**: Written parquet files are never validated to ensure they are complete and readable.

**Impact**:
- Corrupted files may go undetected until database load time
- Silent data loss if files are partially written
- Difficult to identify which batch caused the problem

**Mitigation**:
- Added optional validation in `ParquetBatchManager.write_batch_file()`
- Validates row count matches expected
- Validates file is readable by pandas
- Automatically removes invalid files

### 4. **Race Conditions in Deduplication** (LOW RISK)

**Description**: Multiple batch files may contain duplicate records. Deduplication only happens at database load time using `ROW_NUMBER() OVER(PARTITION BY ...)`.

**Impact**:
- Wasted disk space from duplicate records
- Slower parquet reading during database load
- Increased memory usage during deduplication

**Mitigation**:
- Database loader already handles this with deduplication queries
- Added `validate_no_duplicates()` in database validation to catch issues
- Documented as expected behavior in batch write system

### 5. **Process Crashes Between Parsing and Writing** (MEDIUM RISK)

**Description**: Current architecture uses separate executor pools for parsing and writing. If parsing finishes but writer process crashes, parsed data is lost.

**Current Code**:
```python
with ProcessPoolExecutor(max_workers=1) as writer_executor, \
     ThreadPoolExecutor(max_workers=max_parsing_workers) as parsing_executor:
    
    # Parse data (in memory)
    parsed_data = future.result()
    
    # Submit to writer process
    writer_future = writer_executor.submit(process_batch_to_parquet, batch_data, parquet_dir)
    
    # If writer crashes here, parsed_data is lost
```

**Impact**:
- Need to re-parse from JSON (expensive I/O operation)
- Wasted CPU cycles
- Extended recovery time

**Mitigation**:
- Writer process waits for completion before clearing queue: `writer_future.result()`
- Final check ensures last batch completes: `if writer_future: writer_future.result()`
- Errors are logged with full stack traces for debugging
- CIK tracking prevents re-processing completed CIKs on restart

### 6. **No Database Validation in Testing** (MEDIUM RISK)

**Description**: No comprehensive testing of database integrity, referential constraints, or data quality.

**Impact**:
- Data integrity issues may not be caught until production
- Difficult to verify that data pipeline is working correctly
- Manual validation is time-consuming and error-prone

**Mitigation**:
- Created `utils/db_validation.py` with comprehensive validation framework
- Added `DatabaseValidator` class with methods for:
  - Schema validation (tables, columns, primary keys)
  - Data integrity (nulls, duplicates, ranges)
  - Referential integrity (foreign keys)
  - Data quality (row counts, date consistency)
- Created `tests/test_database_validation.py` with 25+ test cases
- Added pytest fixtures for easy database validation in any test

---

## Implemented Safeguards

### 1. ParquetBatchManager Class

**Location**: `utils/parquet_manager.py`

**Features**:
- Atomic batch operations with commit/rollback
- File validation after write
- Batch ID tracking for related files
- Automatic cleanup of invalid files

**Usage**:
```python
manager = ParquetBatchManager(parquet_dir)

# Write files
file1 = manager.write_batch_file(df_companies, "companies", validate=True)
file2 = manager.write_batch_file(df_filings, "filings", validate=True)

if file1 and file2:
    manager.commit_batch()  # Keep files
else:
    manager.rollback_batch()  # Delete all files in batch
```

### 2. Cleanup Functions

**Location**: `utils/parquet_manager.py`

- `cleanup_partial_batches()`: Remove incomplete batches older than threshold
- `validate_parquet_directory()`: Validate all files in a directory
- `get_batch_ids_in_directory()`: Identify which batches are present

### 3. Database Validation Framework

**Location**: `utils/db_validation.py`

**Key Components**:
- `ValidationResult`: Dataclass for validation results
- `DatabaseValidator`: Main validation class with methods:
  - `validate_table_exists()`
  - `validate_columns()`
  - `validate_primary_key()`
  - `validate_no_duplicates()`
  - `validate_no_nulls()`
  - `validate_foreign_key()`
  - `validate_value_range()`
  - `validate_row_count()`
  - `validate_date_consistency()`
- `validate_edgar_database()`: Comprehensive validation suite

### 4. Test Infrastructure

**Location**: `tests/test_database_validation.py`, `tests/conftest.py`

**Fixtures Added**:
- `in_memory_db`: Provides clean in-memory database for testing
- `db_validator`: Provides DatabaseValidator instance
- `sample_edgar_db`: Provides database with sample EDGAR data

**Test Coverage**:
- Schema validation (tables, columns, primary keys)
- Data integrity (duplicates, nulls)
- Referential integrity (foreign keys)
- Data quality (ranges, row counts, date consistency)
- Error handling and edge cases

---

## Recommended Operational Procedures

### 1. Pre-Run Cleanup

Before starting a parse-to-parquet run:

```bash
# Clean up any partial batches from previous failed runs
python -c "
from pathlib import Path
from utils.parquet_manager import cleanup_partial_batches
cleanup_partial_batches(Path('data/output/parquet'), age_hours=24)
"
```

### 2. Post-Run Validation

After completing a parse-to-parquet run:

```bash
# Validate all parquet directories
python -c "
from pathlib import Path
from utils.parquet_manager import validate_parquet_directory

for table in ['companies', 'tickers', 'filings', 'xbrl_facts']:
    results = validate_parquet_directory(Path('data/output/parquet'), table)
    print(f'{table}: {results}')
"
```

### 3. Database Validation

After loading data into database:

```bash
# Run comprehensive database validation
pytest tests/test_database_validation.py -v

# Or programmatically:
python -c "
import duckdb
from utils.db_validation import validate_edgar_database

conn = duckdb.connect('data/output/edgar.duckdb')
validator = validate_edgar_database(conn)

summary = validator.get_summary()
print(f'Validation Results: {summary}')

for result in validator.results:
    if not result.passed:
        print(result)
"
```

### 4. Regular Maintenance

```bash
# Weekly: Clean up old partial batches
# Monthly: Validate database integrity
# Quarterly: Review and optimize batch size settings
```

---

## Testing the Safeguards

Run the new database validation tests:

```bash
# Run all database validation tests
pytest tests/test_database_validation.py -v

# Run specific test class
pytest tests/test_database_validation.py::TestReferentialIntegrityValidation -v

# Run with coverage
pytest tests/test_database_validation.py --cov=utils.db_validation --cov-report=html
```

---

## Future Enhancements

### Short Term (Next Sprint)
1. Add retry logic for transient parquet write failures
2. Implement progress checkpointing to resume from last successful batch
3. Add metrics collection for batch write performance

### Medium Term (Next Quarter)
1. Implement parquet file compression to reduce disk usage
2. Add data quality checks before writing (e.g., CIK format validation)
3. Create monitoring dashboard for parquet pipeline health

### Long Term (6+ Months)
1. Migrate to transactional file system or object storage
2. Implement blue-green deployment for parquet directories
3. Add automated recovery procedures for common failure modes

---

## Conclusion

The parse-to-parquet pipeline now has comprehensive safeguards against data loss and corruption:

✅ **Atomic batch operations** prevent partial writes  
✅ **Validation framework** catches data quality issues  
✅ **Cleanup utilities** remove orphaned files  
✅ **Comprehensive testing** ensures database integrity  
✅ **Clear documentation** for operational procedures  

Risk profile improved from **HIGH** to **LOW** for most failure scenarios.
